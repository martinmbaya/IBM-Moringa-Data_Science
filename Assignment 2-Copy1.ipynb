{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin mbaya\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prima_indians = pd.read_csv(\"prima_indians.csv\")\n",
    "Y = prima_indians.pop(\" Class\")\n",
    "X = prima_indians\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 75\n",
    "BATCH = 128\n",
    "CLASSES = 2   # number of outputs = number of digits\n",
    "OPTIMIZER = 'RMSprop'\n",
    "RESHAPED = len(xtest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "Xtrain = np.array(xtrain).astype('float32')\n",
    "Xtest = np.array(xtest).astype('float32')\n",
    "Ytrain = keras.utils.to_categorical(ytrain, CLASSES)\n",
    "Ytest = keras.utils.to_categorical(ytest, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/75\n",
      "576/576 [==============================] - 1s 1ms/step - loss: 0.2849 - acc: 0.5660 - val_loss: 0.2374 - val_acc: 0.6719\n",
      "Epoch 2/75\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.2267 - acc: 0.6406 - val_loss: 0.2032 - val_acc: 0.6875\n",
      "Epoch 3/75\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.2052 - acc: 0.6753 - val_loss: 0.2051 - val_acc: 0.7188\n",
      "Epoch 4/75\n",
      "576/576 [==============================] - 0s 207us/step - loss: 0.1999 - acc: 0.6979 - val_loss: 0.1965 - val_acc: 0.7135\n",
      "Epoch 5/75\n",
      "576/576 [==============================] - 0s 153us/step - loss: 0.2096 - acc: 0.6528 - val_loss: 0.1958 - val_acc: 0.7188\n",
      "Epoch 6/75\n",
      "576/576 [==============================] - 0s 197us/step - loss: 0.1948 - acc: 0.6892 - val_loss: 0.2243 - val_acc: 0.6146\n",
      "Epoch 7/75\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.1838 - acc: 0.7309 - val_loss: 0.1947 - val_acc: 0.6979\n",
      "Epoch 8/75\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.1957 - acc: 0.6840 - val_loss: 0.2125 - val_acc: 0.6771\n",
      "Epoch 9/75\n",
      "576/576 [==============================] - 0s 111us/step - loss: 0.1838 - acc: 0.7326 - val_loss: 0.1987 - val_acc: 0.7135\n",
      "Epoch 10/75\n",
      "576/576 [==============================] - 0s 118us/step - loss: 0.1762 - acc: 0.7465 - val_loss: 0.1917 - val_acc: 0.7292\n",
      "Epoch 11/75\n",
      "576/576 [==============================] - 0s 124us/step - loss: 0.1728 - acc: 0.7413 - val_loss: 0.1872 - val_acc: 0.7240\n",
      "Epoch 12/75\n",
      "576/576 [==============================] - 0s 122us/step - loss: 0.1870 - acc: 0.7274 - val_loss: 0.1878 - val_acc: 0.7292\n",
      "Epoch 13/75\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.1862 - acc: 0.7240 - val_loss: 0.2663 - val_acc: 0.5729\n",
      "Epoch 14/75\n",
      "576/576 [==============================] - 0s 113us/step - loss: 0.1906 - acc: 0.7101 - val_loss: 0.1986 - val_acc: 0.7031\n",
      "Epoch 15/75\n",
      "576/576 [==============================] - 0s 117us/step - loss: 0.1682 - acc: 0.7500 - val_loss: 0.2421 - val_acc: 0.5885\n",
      "Epoch 16/75\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.1890 - acc: 0.7014 - val_loss: 0.1852 - val_acc: 0.7344\n",
      "Epoch 17/75\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.1639 - acc: 0.7552 - val_loss: 0.2452 - val_acc: 0.5990\n",
      "Epoch 18/75\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.1875 - acc: 0.7118 - val_loss: 0.1846 - val_acc: 0.7344\n",
      "Epoch 19/75\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.1726 - acc: 0.746 - 0s 186us/step - loss: 0.1684 - acc: 0.7500 - val_loss: 0.2390 - val_acc: 0.5938\n",
      "Epoch 20/75\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.1805 - acc: 0.7274 - val_loss: 0.1836 - val_acc: 0.7344\n",
      "Epoch 21/75\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.1604 - acc: 0.7743 - val_loss: 0.1874 - val_acc: 0.7188\n",
      "Epoch 22/75\n",
      "576/576 [==============================] - 0s 111us/step - loss: 0.1680 - acc: 0.7517 - val_loss: 0.1989 - val_acc: 0.6771\n",
      "Epoch 23/75\n",
      "576/576 [==============================] - 0s 144us/step - loss: 0.1692 - acc: 0.7569 - val_loss: 0.2084 - val_acc: 0.6771\n",
      "Epoch 24/75\n",
      "576/576 [==============================] - 0s 113us/step - loss: 0.1727 - acc: 0.7274 - val_loss: 0.1838 - val_acc: 0.7292\n",
      "Epoch 25/75\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.1595 - acc: 0.7691 - val_loss: 0.2426 - val_acc: 0.5781\n",
      "Epoch 26/75\n",
      "576/576 [==============================] - 0s 71us/step - loss: 0.1780 - acc: 0.7431 - val_loss: 0.1834 - val_acc: 0.7188\n",
      "Epoch 27/75\n",
      "576/576 [==============================] - 0s 121us/step - loss: 0.1692 - acc: 0.7604 - val_loss: 0.1884 - val_acc: 0.7188\n",
      "Epoch 28/75\n",
      "576/576 [==============================] - 0s 50us/step - loss: 0.1529 - acc: 0.7830 - val_loss: 0.1809 - val_acc: 0.7396\n",
      "Epoch 29/75\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.1740 - acc: 0.7500 - val_loss: 0.1816 - val_acc: 0.7396\n",
      "Epoch 30/75\n",
      "576/576 [==============================] - 0s 38us/step - loss: 0.1604 - acc: 0.7674 - val_loss: 0.1945 - val_acc: 0.7188\n",
      "Epoch 31/75\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.1642 - acc: 0.7552 - val_loss: 0.1908 - val_acc: 0.7292\n",
      "Epoch 32/75\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.1715 - acc: 0.7413 - val_loss: 0.1893 - val_acc: 0.7188\n",
      "Epoch 33/75\n",
      "576/576 [==============================] - 0s 32us/step - loss: 0.1606 - acc: 0.7656 - val_loss: 0.2649 - val_acc: 0.5781\n",
      "Epoch 34/75\n",
      "576/576 [==============================] - 0s 23us/step - loss: 0.1749 - acc: 0.7205 - val_loss: 0.1827 - val_acc: 0.7240\n",
      "Epoch 35/75\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.1511 - acc: 0.7812 - val_loss: 0.2232 - val_acc: 0.6562\n",
      "Epoch 36/75\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.1706 - acc: 0.7465 - val_loss: 0.1845 - val_acc: 0.7188\n",
      "Epoch 37/75\n",
      "576/576 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.7847 - val_loss: 0.1975 - val_acc: 0.6875\n",
      "Epoch 38/75\n",
      "576/576 [==============================] - 0s 43us/step - loss: 0.1777 - acc: 0.7188 - val_loss: 0.2167 - val_acc: 0.6510\n",
      "Epoch 39/75\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.1591 - acc: 0.7656 - val_loss: 0.1830 - val_acc: 0.7292\n",
      "Epoch 40/75\n",
      "576/576 [==============================] - 0s 25us/step - loss: 0.1533 - acc: 0.7743 - val_loss: 0.1868 - val_acc: 0.7240\n",
      "Epoch 41/75\n",
      "576/576 [==============================] - 0s 36us/step - loss: 0.1547 - acc: 0.7691 - val_loss: 0.1975 - val_acc: 0.7448\n",
      "Epoch 42/75\n",
      "576/576 [==============================] - 0s 28us/step - loss: 0.1861 - acc: 0.7135 - val_loss: 0.1893 - val_acc: 0.7240\n",
      "Epoch 43/75\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.1493 - acc: 0.7951 - val_loss: 0.1842 - val_acc: 0.7292\n",
      "Epoch 44/75\n",
      "576/576 [==============================] - 0s 30us/step - loss: 0.1739 - acc: 0.7361 - val_loss: 0.1885 - val_acc: 0.7240\n",
      "Epoch 45/75\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.1580 - acc: 0.7708 - val_loss: 0.2134 - val_acc: 0.6719\n",
      "Epoch 46/75\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.1590 - acc: 0.7691 - val_loss: 0.1866 - val_acc: 0.7135\n",
      "Epoch 47/75\n",
      "576/576 [==============================] - 0s 45us/step - loss: 0.1527 - acc: 0.7847 - val_loss: 0.1829 - val_acc: 0.7396\n",
      "Epoch 48/75\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.1730 - acc: 0.7413 - val_loss: 0.2199 - val_acc: 0.6562\n",
      "Epoch 49/75\n",
      "576/576 [==============================] - 0s 130us/step - loss: 0.1621 - acc: 0.7795 - val_loss: 0.2430 - val_acc: 0.6250\n",
      "Epoch 50/75\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.1641 - acc: 0.7639 - val_loss: 0.1824 - val_acc: 0.7292\n",
      "Epoch 51/75\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.1708 - acc: 0.7413 - val_loss: 0.2243 - val_acc: 0.6562\n",
      "Epoch 52/75\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.1521 - acc: 0.7604 - val_loss: 0.1825 - val_acc: 0.7396\n",
      "Epoch 53/75\n",
      "576/576 [==============================] - 0s 55us/step - loss: 0.1453 - acc: 0.7899 - val_loss: 0.1850 - val_acc: 0.7396\n",
      "Epoch 54/75\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.1591 - acc: 0.7795 - val_loss: 0.2353 - val_acc: 0.6354\n",
      "Epoch 55/75\n",
      "576/576 [==============================] - 0s 69us/step - loss: 0.1491 - acc: 0.7917 - val_loss: 0.1920 - val_acc: 0.7188\n",
      "Epoch 56/75\n",
      "576/576 [==============================] - 0s 133us/step - loss: 0.1449 - acc: 0.7934 - val_loss: 0.2055 - val_acc: 0.6927\n",
      "Epoch 57/75\n",
      "576/576 [==============================] - 0s 111us/step - loss: 0.1703 - acc: 0.7500 - val_loss: 0.1997 - val_acc: 0.7135\n",
      "Epoch 58/75\n",
      "576/576 [==============================] - 0s 168us/step - loss: 0.1796 - acc: 0.7188 - val_loss: 0.2223 - val_acc: 0.6719\n",
      "Epoch 59/75\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.1569 - acc: 0.7656 - val_loss: 0.1813 - val_acc: 0.7240\n",
      "Epoch 60/75\n",
      "576/576 [==============================] - 0s 56us/step - loss: 0.1613 - acc: 0.7552 - val_loss: 0.1969 - val_acc: 0.7031\n",
      "Epoch 61/75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 61us/step - loss: 0.1551 - acc: 0.7674 - val_loss: 0.2550 - val_acc: 0.6094\n",
      "Epoch 62/75\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.1560 - acc: 0.7604 - val_loss: 0.2094 - val_acc: 0.6615\n",
      "Epoch 63/75\n",
      "576/576 [==============================] - 0s 47us/step - loss: 0.1461 - acc: 0.7847 - val_loss: 0.1946 - val_acc: 0.7188\n",
      "Epoch 64/75\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.1741 - acc: 0.7413 - val_loss: 0.1991 - val_acc: 0.6927\n",
      "Epoch 65/75\n",
      "576/576 [==============================] - 0s 66us/step - loss: 0.1563 - acc: 0.7708 - val_loss: 0.1856 - val_acc: 0.7240\n",
      "Epoch 66/75\n",
      "576/576 [==============================] - 0s 63us/step - loss: 0.1511 - acc: 0.7812 - val_loss: 0.2006 - val_acc: 0.6875\n",
      "Epoch 67/75\n",
      "576/576 [==============================] - 0s 68us/step - loss: 0.1415 - acc: 0.7969 - val_loss: 0.1796 - val_acc: 0.7604\n",
      "Epoch 68/75\n",
      "576/576 [==============================] - 0s 52us/step - loss: 0.1418 - acc: 0.8021 - val_loss: 0.1976 - val_acc: 0.7083\n",
      "Epoch 69/75\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.1576 - acc: 0.7708 - val_loss: 0.1989 - val_acc: 0.7344\n",
      "Epoch 70/75\n",
      "576/576 [==============================] - 0s 41us/step - loss: 0.1721 - acc: 0.7448 - val_loss: 0.2192 - val_acc: 0.6615\n",
      "Epoch 71/75\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.1608 - acc: 0.7483 - val_loss: 0.1817 - val_acc: 0.7344\n",
      "Epoch 72/75\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.1410 - acc: 0.7986 - val_loss: 0.2059 - val_acc: 0.7292\n",
      "Epoch 73/75\n",
      "576/576 [==============================] - 0s 39us/step - loss: 0.1623 - acc: 0.7535 - val_loss: 0.2071 - val_acc: 0.6927\n",
      "Epoch 74/75\n",
      "576/576 [==============================] - 0s 40us/step - loss: 0.1713 - acc: 0.7309 - val_loss: 0.1824 - val_acc: 0.7500\n",
      "Epoch 75/75\n",
      "576/576 [==============================] - 0s 42us/step - loss: 0.1671 - acc: 0.7448 - val_loss: 0.1931 - val_acc: 0.7292\n",
      "192/192 [==============================] - 0s 35us/step\n",
      "Test loss: 0.19313682615756989\n",
      "Test accuracy: 0.7291666666666666\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, input_dim=RESHAPED))\n",
    "model.add(Activation('sigmoid')) \n",
    "model.add(Dense(2)) \n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer=OPTIMIZER, metrics=['accuracy']) #We will use a slightly more advanced optimiser call Adam. We can discuss how it works in later classes.\n",
    "\n",
    "history = model.fit(Xtrain, Ytrain,\n",
    "                    batch_size=BATCH,\n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=1,\n",
    "                    validation_data=(Xtest, Ytest))\n",
    "score = model.evaluate(Xtest, Ytest, verbose=1) \n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
